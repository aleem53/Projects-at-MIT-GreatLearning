{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWKrvyhfVL3e"
   },
   "source": [
    "# **Milestone 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rzh8pcvmtOS8"
   },
   "source": [
    "## **Model Building**\n",
    "\n",
    "1. What we want to predict is the \"Price\". We will use the normalized version 'price_log' for modeling.\n",
    "2. Before we proceed to the model, we'll have to encode categorical features. We will drop categorical features like - Name \n",
    "3. We'll split the data into train and test, to be able to evaluate the model that we build on the train data.\n",
    "4. Build Regression models using train data.\n",
    "5. Evaluate the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0L-oAMItxLP-"
   },
   "source": [
    "### **Split Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aat-Ne-ZVL3e"
   },
   "source": [
    "<li>Step1: Split the data into X and Y . \n",
    "<li>Step2: Encode the categorical variables in X using pd.dummies.\n",
    "<li>Step3: Split the data into train and test using train_test_split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cwh_IhfqVL3f"
   },
   "source": [
    "<b>Think about it:</b> Why we should drop 'Name','Price','price_log','Kilometers_Driven' from X before splitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "id": "NTly1jIxtOS8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Name    Location  Year  \\\n",
      "0                                Maruti Wagon R LXI CNG      Mumbai  2010   \n",
      "1                      Hyundai Creta 1.6 CRDi SX Option        Pune  2015   \n",
      "2                                          Honda Jazz V     Chennai  2011   \n",
      "3                                     Maruti Ertiga VDI     Chennai  2012   \n",
      "4                       Audi A4 New 2.0 TDI Multitronic  Coimbatore  2013   \n",
      "...                                                 ...         ...   ...   \n",
      "7248                  Volkswagen Vento Diesel Trendline   Hyderabad  2011   \n",
      "7249                             Volkswagen Polo GT TSI      Mumbai  2015   \n",
      "7250                             Nissan Micra Diesel XV     Kolkata  2012   \n",
      "7251                             Volkswagen Polo GT TSI        Pune  2013   \n",
      "7252  Mercedes-Benz E-Class 2009-2013 E 220 CDI Avan...       Kochi  2014   \n",
      "\n",
      "      Kilometers_Driven Fuel_Type Transmission Owner_Type  Mileage  Engine  \\\n",
      "0                 72000       CNG       Manual      First    26.60   998.0   \n",
      "1                 41000    Diesel       Manual      First    19.67  1582.0   \n",
      "2                 46000    Petrol       Manual      First    18.20  1199.0   \n",
      "3                 87000    Diesel       Manual      First    20.77  1248.0   \n",
      "4                 40670    Diesel    Automatic     Second    15.20  1968.0   \n",
      "...                 ...       ...          ...        ...      ...     ...   \n",
      "7248              89411    Diesel       Manual      First    20.54  1598.0   \n",
      "7249              59000    Petrol    Automatic      First    17.21  1197.0   \n",
      "7250              28000    Diesel       Manual      First    23.08  1461.0   \n",
      "7251              52262    Petrol    Automatic      Third    17.20  1197.0   \n",
      "7252              72443    Diesel    Automatic      First    10.00  2148.0   \n",
      "\n",
      "       Power  Seats  Price  \n",
      "0      58.16    5.0   1.75  \n",
      "1     126.20    5.0  12.50  \n",
      "2      88.70    5.0   4.50  \n",
      "3      88.76    7.0   6.00  \n",
      "4     140.80    5.0  17.74  \n",
      "...      ...    ...    ...  \n",
      "7248  103.60    5.0   5.64  \n",
      "7249  103.60    5.0   5.64  \n",
      "7250   63.10    5.0   5.64  \n",
      "7251  103.60    5.0   5.64  \n",
      "7252  170.00    5.0   5.64  \n",
      "\n",
      "[7252 rows x 12 columns]\n",
      "Name                 0\n",
      "Location             0\n",
      "Year                 0\n",
      "Fuel_Type            0\n",
      "Transmission         0\n",
      "Kilometers_Driven    0\n",
      "Owner_Type           0\n",
      "Seats                0\n",
      "Engine               0\n",
      "Power                0\n",
      "Mileage              0\n",
      "Price                0\n",
      "price_log            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Initial Step of data file reading to cards_data df\n",
    "\n",
    "#Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#to ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Loading data into cars_data DataFrame from the used_cars.csv data file\n",
    "cars_data = pd.read_csv('used_cars.csv')\n",
    " \n",
    "# Data Imputing for missing values - NaN to Median values for the following columns\n",
    "# Impute missing values in Price, you can use fillna method in pandas\n",
    "cars_data['Price'] = cars_data['Price'].fillna(cars_data['Price'].median())\n",
    "\n",
    "# Impute missing values in Seats, you can use fillna method in pandas\n",
    "cars_data['Seats'] = cars_data['Seats'].fillna(cars_data['Seats'].median())\n",
    "\n",
    "# Impute missing values in Mileage, you can use fillna method in pandas\n",
    "cars_data['Mileage'] = cars_data['Mileage'].fillna(cars_data['Mileage'].median())\n",
    "\n",
    "# Impute missing values in Power, you can use fillna method in pandas\n",
    "cars_data['Power'] = cars_data['Power'].fillna(cars_data['Power'].median())\n",
    "\n",
    "# Impute missing values in Engine, you can use fillna method in pandas\n",
    "cars_data['Engine'] = cars_data['Engine'].fillna(cars_data['Engine'].median())\n",
    "\n",
    "# Remove S.No. and New_price columns from data which are not needed for Regression Analysis \n",
    "cars_data.drop('S.No.', inplace=True, axis=1)\n",
    "cars_data.drop('New_price', inplace=True, axis=1)\n",
    " \n",
    "\n",
    "# Removing the 'row' at index 2328 from the data. Hint: use the argument inplace=True\n",
    "cars_data.drop([2328],inplace = True )\n",
    "\n",
    "#Print Final cleaned up data set\n",
    "print (cars_data)\n",
    "\n",
    "# We can add a log transformed kilometers_driven feature in data\n",
    "cars_data[\"kilometers_driven_log\"] = np.log(cars_data[\"Kilometers_Driven\"])\n",
    "\n",
    "# We can Add a log transformed Price feature in data\n",
    "cars_data[\"price_log\"] = np.log(cars_data[\"Price\"])\n",
    " \n",
    "#Check if any columns still has any NaN values \n",
    "col=['Name', 'Location', 'Year', 'Fuel_Type','Transmission','Kilometers_Driven', 'Owner_Type', 'Seats', 'Engine','Power','Mileage','Price', 'price_log']\n",
    "print (cars_data[col].isnull().sum())\n",
    "\n",
    "#cars_data =cars_data.astype(float)\n",
    "\n",
    "# Remove the limit from the number of displayed columns and rows. It helps to see the entire dataframe while printing it\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "\n",
    "# Step-1\n",
    "X = cars_data.drop(['Name','Price','price_log','Kilometers_Driven'],axis=1)\n",
    "y = cars_data[['price_log', 'Price']]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "id": "vzCLGMzbVL3f"
   },
   "outputs": [],
   "source": [
    "# Step-2 Use pd.get_dummies(drop_first=True)\n",
    "X = pd.get_dummies(X,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "id": "JqVHLEHVRRKK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5076, 24) (2176, 24)\n"
     ]
    }
   ],
   "source": [
    "# Step-3 Splitting data into training and test set:\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "id": "glAH2vMFtOS8"
   },
   "outputs": [],
   "source": [
    "# Let us write a function for calculating r2_score and RMSE on train and test data.\n",
    "# This function takes model as an input on which we have trained particular algorithm.\n",
    "#the categorical column as the input and returns the boxplots and histograms for the variable.\n",
    "import sklearn.metrics as metrics\n",
    "def get_model_score(model, flag=True):\n",
    "    '''\n",
    "    model : regressor to predict values of X\n",
    "\n",
    "    '''\n",
    "    # defining an empty list to store train and test results\n",
    "    score_list=[] \n",
    "    \n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_train_ = np.exp(pred_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "    pred_test_ = np.exp(pred_test)\n",
    "    \n",
    "    train_r2=metrics.r2_score(y_train['Price'],pred_train_)\n",
    "    test_r2=metrics.r2_score(y_test['Price'],pred_test_)\n",
    "    train_rmse=metrics.mean_squared_error(y_train['Price'],pred_train_,squared=False)\n",
    "    test_rmse=metrics.mean_squared_error(y_test['Price'],pred_test_,squared=False)\n",
    "    \n",
    "    #Adding all scores in the list\n",
    "    score_list.extend((train_r2,test_r2,train_rmse,test_rmse))\n",
    "    \n",
    "    # If the flag is set to True then only the following print statements will be dispayed, the default value is True\n",
    "    if flag==True: \n",
    "        print(\"R-sqaure on training set : \",metrics.r2_score(y_train['Price'],pred_train_))\n",
    "        print(\"R-square on test set : \",metrics.r2_score(y_test['Price'],pred_test_))\n",
    "        print(\"RMSE on training set : \",np.sqrt(metrics.mean_squared_error(y_train['Price'],pred_train_)))\n",
    "        print(\"RMSE on test set : \",np.sqrt(metrics.mean_squared_error(y_test['Price'],pred_test_)))\n",
    "    \n",
    "    # returning the list with train and test scores\n",
    "    return score_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8qcI692VL3g"
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaj2riZFVL3g"
   },
   "source": [
    "For Regression Problems, some of the algorithms used are :<br>\n",
    "\n",
    "**1) Linear Regression** <br>\n",
    "**2) Ridge / Lasso Regression** <br>\n",
    "**3) Decision Trees** <br>\n",
    "**4) Random Forest** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwL33RaztOS9"
   },
   "source": [
    "### **Fitting a linear model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXj-84YCVL3h"
   },
   "source": [
    "Linear Regression can be implemented using: <br>\n",
    "\n",
    "**1) Sklearn:** https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html <br>\n",
    "**2) Statsmodels:** https://www.statsmodels.org/stable/regression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "id": "tABeKbbNVL3h"
   },
   "outputs": [],
   "source": [
    "# import Linear Regression from sklearn\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "id": "FT3gcKDetOS9"
   },
   "outputs": [],
   "source": [
    "# Create a linear regression model\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "id": "rMmX-FJatOS9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit linear regression model\n",
    "lr.fit(X_train,y_train['price_log']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "id": "ABshmMPAtOS9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-sqaure on training set :  0.5628499237591433\n",
      "R-square on test set :  0.7034208186923349\n",
      "RMSE on training set :  6.778145833783173\n",
      "RMSE on test set :  5.619982998714185\n"
     ]
    }
   ],
   "source": [
    "# Get score of the model.\n",
    "LR_score = get_model_score(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgNkZ0HctOS9"
   },
   "source": [
    "#### **Observations from results: _____**\n",
    "Here we see that the R-squared is higher (0.703) for the test data than training data set where it is 0.563. The RSME(Root Mean Squared Error) is higher(6.78) than the same for the test data set where it is 5.62. The lower the RSME is, the better the model is. So, this model is better for theh test set rather training. However, from higher R-squared value for training data set, it felt the model fits better for the training data set. As an RSME between 0.2 to 0.5 is a good indicator of the model fitting data, the model is a good fit for both of training and test data; both scores are higher than 0.5 actually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNN1nlPqIrdC"
   },
   "source": [
    "#### **Important variables of Linear Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyKxdVyhVL3i"
   },
   "source": [
    "Building a model using statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "id": "vHyqhuVMIrdC",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              price_log   R-squared:                       0.731\n",
      "Model:                            OLS   Adj. R-squared:                  0.729\n",
      "Method:                 Least Squares   F-statistic:                     595.5\n",
      "Date:                Sat, 04 Jun 2022   Prob (F-statistic):               0.00\n",
      "Time:                        10:39:47   Log-Likelihood:                -2707.4\n",
      "No. Observations:                5076   AIC:                             5463.\n",
      "Df Residuals:                    5052   BIC:                             5620.\n",
      "Df Model:                          23                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=============================================================================================\n",
      "                                coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "const                      -196.6493      5.319    -36.969      0.000    -207.077    -186.221\n",
      "Year                          0.0986      0.003     37.595      0.000       0.093       0.104\n",
      "Mileage                      -0.0130      0.002     -6.355      0.000      -0.017      -0.009\n",
      "Engine                        0.0002   2.72e-05      7.201      0.000       0.000       0.000\n",
      "Power                         0.0056      0.000     20.584      0.000       0.005       0.006\n",
      "Seats                         0.0004      0.010      0.042      0.966      -0.018       0.019\n",
      "kilometers_driven_log        -0.0619      0.011     -5.748      0.000      -0.083      -0.041\n",
      "Location_Bangalore            0.0980      0.038      2.556      0.011       0.023       0.173\n",
      "Location_Chennai              0.0098      0.036      0.270      0.787      -0.061       0.081\n",
      "Location_Coimbatore           0.0604      0.035      1.719      0.086      -0.009       0.129\n",
      "Location_Delhi               -0.0231      0.035     -0.651      0.515      -0.093       0.046\n",
      "Location_Hyderabad            0.0840      0.034      2.464      0.014       0.017       0.151\n",
      "Location_Jaipur              -0.0549      0.037     -1.472      0.141      -0.128       0.018\n",
      "Location_Kochi               -0.0362      0.035     -1.032      0.302      -0.105       0.033\n",
      "Location_Kolkata             -0.1968      0.036     -5.496      0.000      -0.267      -0.127\n",
      "Location_Mumbai              -0.0281      0.034     -0.824      0.410      -0.095       0.039\n",
      "Location_Pune                -0.0334      0.035     -0.954      0.340      -0.102       0.035\n",
      "Fuel_Type_Diesel              0.2069      0.061      3.369      0.001       0.086       0.327\n",
      "Fuel_Type_Electric         2.124e-17   1.82e-16      0.116      0.907   -3.36e-16    3.79e-16\n",
      "Fuel_Type_LPG                 0.0176      0.168      0.105      0.917      -0.312       0.347\n",
      "Fuel_Type_Petrol             -0.0480      0.062     -0.774      0.439      -0.170       0.074\n",
      "Transmission_Manual          -0.2543      0.018    -14.421      0.000      -0.289      -0.220\n",
      "Owner_Type_Fourth & Above     0.2683      0.132      2.033      0.042       0.010       0.527\n",
      "Owner_Type_Second            -0.0347      0.017     -2.003      0.045      -0.069      -0.001\n",
      "Owner_Type_Third             -0.1835      0.044     -4.170      0.000      -0.270      -0.097\n",
      "==============================================================================\n",
      "Omnibus:                      781.749   Durbin-Watson:                   2.029\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             4305.623\n",
      "Skew:                          -0.621   Prob(JB):                         0.00\n",
      "Kurtosis:                       7.338   Cond. No.                     8.22e+19\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 5.14e-30. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "# Import Statsmodels \n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Statsmodel api does not add a constant by default. We need to add it explicitly.\n",
    "x_train = sm.add_constant(X_train)\n",
    "# Add constant to test data\n",
    "x_test = sm.add_constant(X_test)\n",
    "\n",
    "def build_ols_model(train):\n",
    "    # Create the model\n",
    "    olsmodel = sm.OLS(y_train[\"price_log\"], train)\n",
    "    return olsmodel.fit()\n",
    "\n",
    "\n",
    "# Fit linear model on new dataset\n",
    "olsmodel1 = build_ols_model(x_train)\n",
    "print(olsmodel1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "id": "C01l5EucVL3i"
   },
   "outputs": [],
   "source": [
    "# Retrive Coeff values, p-values and store them in the dataframe\n",
    "olsmod = pd.DataFrame(olsmodel1.params, columns=['coef'])\n",
    "olsmod['pval']=olsmodel1.pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "id": "FbQQZacV9WMm"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>pval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Owner_Type_Second</th>\n",
       "      <td>-0.034730</td>\n",
       "      <td>4.520219e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Owner_Type_Fourth &amp; Above</th>\n",
       "      <td>0.268276</td>\n",
       "      <td>4.212803e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_Hyderabad</th>\n",
       "      <td>0.084020</td>\n",
       "      <td>1.378133e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_Bangalore</th>\n",
       "      <td>0.097989</td>\n",
       "      <td>1.062837e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fuel_Type_Diesel</th>\n",
       "      <td>0.206907</td>\n",
       "      <td>7.608686e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Owner_Type_Third</th>\n",
       "      <td>-0.183473</td>\n",
       "      <td>3.096205e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_Kolkata</th>\n",
       "      <td>-0.196807</td>\n",
       "      <td>4.085906e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kilometers_driven_log</th>\n",
       "      <td>-0.061878</td>\n",
       "      <td>9.571330e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mileage</th>\n",
       "      <td>-0.012983</td>\n",
       "      <td>2.271471e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Engine</th>\n",
       "      <td>0.000196</td>\n",
       "      <td>6.849539e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Transmission_Manual</th>\n",
       "      <td>-0.254349</td>\n",
       "      <td>3.115008e-46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power</th>\n",
       "      <td>0.005579</td>\n",
       "      <td>1.784262e-90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>-196.649277</td>\n",
       "      <td>5.264924e-265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>0.098605</td>\n",
       "      <td>5.887979e-273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 coef           pval\n",
       "Owner_Type_Second           -0.034730   4.520219e-02\n",
       "Owner_Type_Fourth & Above    0.268276   4.212803e-02\n",
       "Location_Hyderabad           0.084020   1.378133e-02\n",
       "Location_Bangalore           0.097989   1.062837e-02\n",
       "Fuel_Type_Diesel             0.206907   7.608686e-04\n",
       "Owner_Type_Third            -0.183473   3.096205e-05\n",
       "Location_Kolkata            -0.196807   4.085906e-08\n",
       "kilometers_driven_log       -0.061878   9.571330e-09\n",
       "Mileage                     -0.012983   2.271471e-10\n",
       "Engine                       0.000196   6.849539e-13\n",
       "Transmission_Manual         -0.254349   3.115008e-46\n",
       "Power                        0.005579   1.784262e-90\n",
       "const                     -196.649277  5.264924e-265\n",
       "Year                         0.098605  5.887979e-273"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FIlter by significant p-value (pval <0.05) and sort descending by Odds ratio\n",
    "olsmod = olsmod.sort_values(by=\"pval\", ascending=False)\n",
    "pval_filter = olsmod['pval']<=0.05\n",
    "olsmod[pval_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "id": "yjplRhssIrdC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mMost overall significant categorical varaibles of LINEAR REGRESSION  are \u001b[95m :\n",
      " ['Owner_Type', 'Location', 'Fuel_Type', 'kilometers_driven_log', 'Mileage', 'Engine', 'Transmission', 'Power', 'Year']\n"
     ]
    }
   ],
   "source": [
    "# we are looking are overall significant varaible\n",
    "pval_filter = olsmod['pval']<=0.05\n",
    "imp_vars = olsmod[pval_filter].index.tolist()\n",
    "\n",
    "# we are going to get overall varaibles (un-one-hot encoded varables) from categorical varaibles\n",
    "sig_var = []\n",
    "for col in imp_vars:\n",
    "    if '' in col:\n",
    "        first_part = col.split('_')[0]\n",
    "        for c in cars_data.columns:\n",
    "            if first_part in c and c not in sig_var :\n",
    "                sig_var.append(c)\n",
    " \n",
    "\n",
    "start = '\\033[1m'\n",
    "end = '\\033[95m'\n",
    "print(start+'Most overall significant categorical varaibles of LINEAR REGRESSION  are '+end,':\\n',sig_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uubmKLlVL3j"
   },
   "source": [
    "<b>Build Ridge / Lasso Regression similar to Linear Regression:</b><br>\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "id": "9--bgDNhVL3j"
   },
   "outputs": [],
   "source": [
    "# import Ridge/ Lasso Regression from sklearn\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "id": "TwSuk8Q9VL3j"
   },
   "outputs": [],
   "source": [
    "# Create a Ridge regression model, alpha=1.0 for full penalty\n",
    "ridge_lasso = Ridge(alpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "id": "5cWh1QK0VL3j"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge()"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Ridge regression model.\n",
    "ridge_lasso.fit(X_train,y_train['price_log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "id": "c2tIACYOVL3j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-sqaure on training set :  0.562671552976983\n",
      "R-square on test set :  0.7032861344323269\n",
      "RMSE on training set :  6.7795285395575675\n",
      "RMSE on test set :  5.621258943527664\n"
     ]
    }
   ],
   "source": [
    "# Get score of the model.\n",
    "Ridge_Score=get_model_score(ridge_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jUB9svHjVL3j"
   },
   "outputs": [],
   "source": [
    "# My Obserations -\n",
    "# This model has similar R-squared values for training data set as well test data set (0.563 and 0.703 respectively), \n",
    "# The RSME values are also very similar to those of Linear Regression model, and same type of comments apply here. \n",
    "# This model is also a strong fit for both of training and test data set.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owyg5IpstOS9"
   },
   "source": [
    "### **Decision Tree** \n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "id": "GjZeRER6VL3k"
   },
   "outputs": [],
   "source": [
    "# import Decision tree for Regression from sklearn\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "id": "shBet9WztOS-"
   },
   "outputs": [],
   "source": [
    "# Create a decision tree regression model\n",
    "Dtree = DecisionTreeRegressor(random_state = 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "id": "eJxVYVXXtOS-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(random_state=1)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit decision tree regression model.\n",
    "Dtree.fit(X_train,y_train['price_log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "id": "vGbEjda0tOS-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-sqaure on training set :  0.9999092296903165\n",
      "R-square on test set :  0.5825526342226197\n",
      "RMSE on training set :  0.09767142812748364\n",
      "RMSE on test set :  6.667538517004757\n"
     ]
    }
   ],
   "source": [
    "# Get score of the model.\n",
    "Dtree_Score = get_model_score(Dtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrCgLVKwtOS-"
   },
   "source": [
    "**Observations from results -**\n",
    "Here we find that R-squared value for the training set is really high (0.999), meaning the training data is a perfect fit for this model, or this model is a perfect fit for the training data set; Howeverver, the test data is not; for test data this model is a good fit also. The RMSE is only 0.097 which is an indicator of the \"perfect fit\" of this model for the training data. However, for the test data, RSME is significantly different (6.66), but still better than the Linear Regression model or the Ridge-Lasso model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6z2V0IwtVL3k"
   },
   "source": [
    "Print the importance of features in the tree building ( The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "id": "E8ro_i9vIrdF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Imp\n",
      "Power                      0.539645\n",
      "Year                       0.209081\n",
      "kilometers_driven_log      0.074535\n",
      "Engine                     0.053221\n",
      "Mileage                    0.043895\n",
      "Transmission_Manual        0.008721\n",
      "Location_Kolkata           0.007074\n",
      "Location_Kochi             0.007052\n",
      "Location_Hyderabad         0.006872\n",
      "Seats                      0.006360\n",
      "Owner_Type_Second          0.005467\n",
      "Location_Coimbatore        0.005242\n",
      "Location_Mumbai            0.004682\n",
      "Location_Bangalore         0.004641\n",
      "Location_Delhi             0.004373\n",
      "Fuel_Type_Diesel           0.004222\n",
      "Location_Pune              0.004189\n",
      "Location_Jaipur            0.002891\n",
      "Location_Chennai           0.002688\n",
      "Fuel_Type_Petrol           0.002366\n",
      "Owner_Type_Third           0.001584\n",
      "Owner_Type_Fourth & Above  0.001126\n",
      "Fuel_Type_LPG              0.000072\n",
      "Fuel_Type_Electric         0.000000\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(Dtree.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9P0pzHHIrdG"
   },
   "source": [
    "#### **Observations and insights: _____**\n",
    "This model has indicated by \"Imp\" values that the 3 key Important features they look at while purchasing a used car are Power, Year of the manufacture of the car and kilometers driven. Engine and Mileage are two other important features they consider for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8eFynaNtOS-"
   },
   "source": [
    "### **Random Forest**\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "id": "Jhw-0FsNVL3l"
   },
   "outputs": [],
   "source": [
    "# import Randomforest for Regression from sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "id": "-4S4FoDXtOS-"
   },
   "outputs": [],
   "source": [
    "# Create a Randomforest regression model \n",
    "Rforest = RandomForestRegressor(n_estimators = 100, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "id": "gBlavhMTtOS-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(random_state=1)"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Randomforest regression model.\n",
    "Rforest.fit(X_train,y_train['price_log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "id": "VLDDeeAGtOS_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-sqaure on training set :  0.591617279038231\n",
      "R-square on test set :  0.3416452818679234\n",
      "RMSE on training set :  6.551327473597888\n",
      "RMSE on test set :  8.37326135201962\n"
     ]
    }
   ],
   "source": [
    "# Get score of the model.\n",
    "Rforest_Score = get_model_score(Rforest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGNTRfaitOS_"
   },
   "source": [
    "**Observations and insights -**\n",
    "From the R-squard values, I would say this model fits for both of trainign data set as well as the test data set; training data set has a better fit; However,  the RSME values are higher here compared to LR and Ridge models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgwyNxUuIrdG"
   },
   "source": [
    "**Feature Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "id": "AWRS7zISIrdG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Imp\n",
      "Power                      0.514132\n",
      "Year                       0.178521\n",
      "kilometers_driven_log      0.096808\n",
      "Engine                     0.058547\n",
      "Mileage                    0.052835\n",
      "Seats                      0.009245\n",
      "Transmission_Manual        0.009130\n",
      "Location_Hyderabad         0.009065\n",
      "Location_Coimbatore        0.009033\n",
      "Location_Kolkata           0.008087\n",
      "Owner_Type_Second          0.007378\n",
      "Location_Kochi             0.006803\n",
      "Location_Bangalore         0.006524\n",
      "Location_Mumbai            0.006114\n",
      "Location_Delhi             0.005348\n",
      "Location_Chennai           0.005335\n",
      "Location_Pune              0.004458\n",
      "Location_Jaipur            0.004359\n",
      "Fuel_Type_Diesel           0.003391\n",
      "Fuel_Type_Petrol           0.003221\n",
      "Owner_Type_Third           0.001255\n",
      "Owner_Type_Fourth & Above  0.000343\n",
      "Fuel_Type_LPG              0.000069\n",
      "Fuel_Type_Electric         0.000000\n"
     ]
    }
   ],
   "source": [
    "# Print important features of Randomforest, similar to decision trees\n",
    "print(pd.DataFrame(Rforest.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cG9ZD9ozIrdH"
   },
   "source": [
    "#### **Observations and insights: _____**\n",
    "This model is also indicating Power, Year and Kilometers driven as the three major Important features for the used car in making a decision on purchasing; the next two features are Engine and Mileage like the previous model had indicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sw0dMSgetOS_"
   },
   "source": [
    "### **Hyperparameter Tuning: Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "id": "4eF0aYHHtOS_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=4, max_features=11, random_state=1)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing DecisionTreeClassifier from sklearn.tree Library\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Importing GridSearchCV from sklearn.model_selection Library\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Choosing the type of estimator. \n",
    "dtree_tuned = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Fit decision tree regression model.\n",
    "y_train['price_log'] = y_train['price_log'].astype(int)\n",
    "dtree_tuned.fit(X_train,y_train['price_log'])\n",
    " \n",
    "\n",
    "# Grid of parameters to choose from.\n",
    "# Check documentation for all the parametrs that the model takes and play with those.\n",
    "params = [{'max_depth': list(range(1, 5)), 'max_features': list(range(0,14))}]\n",
    "    \n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(dtree_tuned, params, cv=4, scoring='r2' )\n",
    "grid_obj = grid_obj.fit(X_train,y_train['price_log'])\n",
    "\n",
    "# Set the clf to the best combination of parameters\n",
    "dtree_tuned = grid_obj.best_estimator_\n",
    "\n",
    "# Fit the best algorithm to the data. \n",
    "dtree_tuned.fit(X_train,y_train['price_log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "id": "hctfJIAXtOS_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-sqaure on training set :  0.3894411353252082\n",
      "R-square on test set :  0.406538636018665\n",
      "RMSE on training set :  8.010495852610374\n",
      "RMSE on test set :  7.94988590807018\n"
     ]
    }
   ],
   "source": [
    "# Get score of the dtree_tuned\n",
    "scorer = get_model_score(dtree_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsGmvq1StOS_"
   },
   "source": [
    "#### **Observations and insights: _____**\n",
    "We find that R-squared values for both of training and test data set are much lower (0.389 and 0.406 respectively) than un-tuned version of Decision Tree model; the RSME values are higher as well for both data set. I am afriad the parameters chosen were incorrect or the tuning did not work well. I feel this tuning did not work as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jssEF5eIrdH"
   },
   "source": [
    "**Feature Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "id": "OdzQWq8WtOTA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Imp\n",
      "Year                       0.319833\n",
      "Transmission_Manual        0.270954\n",
      "Engine                     0.151997\n",
      "Mileage                    0.137495\n",
      "Power                      0.105857\n",
      "kilometers_driven_log      0.013864\n",
      "Location_Mumbai            0.000000\n",
      "Owner_Type_Second          0.000000\n",
      "Owner_Type_Fourth & Above  0.000000\n",
      "Fuel_Type_Petrol           0.000000\n",
      "Fuel_Type_LPG              0.000000\n",
      "Fuel_Type_Electric         0.000000\n",
      "Fuel_Type_Diesel           0.000000\n",
      "Location_Pune              0.000000\n",
      "Location_Kochi             0.000000\n",
      "Location_Kolkata           0.000000\n",
      "Location_Jaipur            0.000000\n",
      "Location_Hyderabad         0.000000\n",
      "Location_Delhi             0.000000\n",
      "Location_Coimbatore        0.000000\n",
      "Location_Chennai           0.000000\n",
      "Location_Bangalore         0.000000\n",
      "Seats                      0.000000\n",
      "Owner_Type_Third           0.000000\n"
     ]
    }
   ],
   "source": [
    "# Print important features of tuned decision tree similar to decision trees\n",
    "print(pd.DataFrame(dtree_tuned.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-r8AR_VotOTB"
   },
   "source": [
    "#### **Observations and insights: _____**\n",
    "In this model, the first three Important features are Year, Tansmission_Manual and Engine; the next two features are Mileage and Power. Noticeable here is the \"Imp\" factor values; much lower than the un-tuned version of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18uxHTy2tOTB"
   },
   "source": [
    "### **Hyperparameter Tuning: Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "id": "4P_Mj0JYtOTC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=4, max_features=12)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Choose the type of Regressor. \n",
    "rforest_tuned = RandomForestClassifier()\n",
    "\n",
    "# Define the parameters for Grid to choose from \n",
    "# Check documentation for all the parametrs that the model takes and play with those\n",
    "params = [{'max_depth': list(range(1, 5)), 'max_features': list(range(0,14))}]\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "# r2_score() is used\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(rforest_tuned, params, cv=4, scoring='r2' )\n",
    "grid_obj = grid_obj.fit(X_train,y_train['price_log'])\n",
    "\n",
    "# Set the clf to the best combination of parameters\n",
    "rforest_tuned = grid_obj.best_estimator_\n",
    "\n",
    "# Fit the best algorithm to the data. \n",
    "rforest_tuned.fit(X_train,y_train['price_log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "id": "HSBtYgpctOTC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-sqaure on training set :  0.35656363363505317\n",
      "R-square on test set :  0.382464735290441\n",
      "RMSE on training set :  8.223343485113766\n",
      "RMSE on test set :  8.109527528735667\n"
     ]
    }
   ],
   "source": [
    "# Get score of the model.\n",
    "scorer = get_model_score(rforest_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1WHqIX9tOTC"
   },
   "source": [
    "#### **Observations and insights: _____**\n",
    "The R-squared values for both of training and test data set are low, but still good enough per definition to say, the model fits the data; however, none of those is a strong fit. The RMSE errors are also pretty high (> 8.0 ) compared to all other models we observed so far. Looks like this is the worst-fit model so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItsgSUyiIrdI"
   },
   "source": [
    "**Feature Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "id": "9khvM2ZhtOTC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Imp\n",
      "Power                      0.362223\n",
      "Year                       0.318962\n",
      "Engine                     0.188191\n",
      "Transmission_Manual        0.050650\n",
      "kilometers_driven_log      0.040351\n",
      "Mileage                    0.019237\n",
      "Seats                      0.007095\n",
      "Fuel_Type_Diesel           0.005915\n",
      "Fuel_Type_Petrol           0.003255\n",
      "Owner_Type_Second          0.001545\n",
      "Location_Kolkata           0.001036\n",
      "Location_Coimbatore        0.000572\n",
      "Owner_Type_Third           0.000522\n",
      "Location_Bangalore         0.000240\n",
      "Location_Kochi             0.000083\n",
      "Location_Hyderabad         0.000079\n",
      "Owner_Type_Fourth & Above  0.000046\n",
      "Location_Jaipur            0.000000\n",
      "Location_Mumbai            0.000000\n",
      "Location_Pune              0.000000\n",
      "Fuel_Type_Electric         0.000000\n",
      "Fuel_Type_LPG              0.000000\n",
      "Location_Delhi             0.000000\n",
      "Location_Chennai           0.000000\n"
     ]
    }
   ],
   "source": [
    "# Print important features of tuned ramdom forest similar to decision trees\n",
    "print(pd.DataFrame(rforest_tuned.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBoHEXnjtOTC"
   },
   "source": [
    "#### **Observations and insights: ______**\n",
    "The three major Important features for a used car per this model are Power, Year and Engine. The next two important factors are Kilometers driven and Mileage. The factor values (scores) are low compared to other models we studied, though.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "id": "DCInk4Y8tOTC"
   },
   "outputs": [],
   "source": [
    "# defining list of models you have trained\n",
    "models = [lr, ridge_lasso, Dtree, dtree_tuned, Rforest, rforest_tuned ]\n",
    "\n",
    "# defining empty lists to add train and test results\n",
    "r2_train = []\n",
    "r2_test = []\n",
    "rmse_train= []\n",
    "rmse_test= []\n",
    "\n",
    "# looping through all the models to get the rmse and r2 scores\n",
    "for model in models:\n",
    "    # accuracy score\n",
    "    j = get_model_score(model,False)\n",
    "    r2_train.append(j[0])\n",
    "    r2_test.append(j[1])\n",
    "    rmse_train.append(j[2])\n",
    "    rmse_test.append(j[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "id": "zuLokC7xtOTD"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train_r2</th>\n",
       "      <th>Test_r2</th>\n",
       "      <th>Train_RMSE</th>\n",
       "      <th>Test_RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>0.562850</td>\n",
       "      <td>0.703421</td>\n",
       "      <td>6.778146</td>\n",
       "      <td>5.619983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge()</td>\n",
       "      <td>0.562672</td>\n",
       "      <td>0.703286</td>\n",
       "      <td>6.779529</td>\n",
       "      <td>5.621259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecisionTreeRegressor(random_state=1)</td>\n",
       "      <td>0.999909</td>\n",
       "      <td>0.582553</td>\n",
       "      <td>0.097671</td>\n",
       "      <td>6.667539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier(max_depth=4, max_featur...</td>\n",
       "      <td>0.389441</td>\n",
       "      <td>0.406539</td>\n",
       "      <td>8.010496</td>\n",
       "      <td>7.949886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(DecisionTreeRegressor(max_features='auto', ra...</td>\n",
       "      <td>0.591617</td>\n",
       "      <td>0.341645</td>\n",
       "      <td>6.551327</td>\n",
       "      <td>8.373261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(DecisionTreeClassifier(max_depth=4, max_featu...</td>\n",
       "      <td>0.352894</td>\n",
       "      <td>0.384961</td>\n",
       "      <td>8.246757</td>\n",
       "      <td>8.093122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model  Train_r2   Test_r2  \\\n",
       "0                                 LinearRegression()  0.562850  0.703421   \n",
       "1                                            Ridge()  0.562672  0.703286   \n",
       "2              DecisionTreeRegressor(random_state=1)  0.999909  0.582553   \n",
       "3  DecisionTreeClassifier(max_depth=4, max_featur...  0.389441  0.406539   \n",
       "4  (DecisionTreeRegressor(max_features='auto', ra...  0.591617  0.341645   \n",
       "5  (DecisionTreeClassifier(max_depth=4, max_featu...  0.352894  0.384961   \n",
       "\n",
       "   Train_RMSE  Test_RMSE  \n",
       "0    6.778146   5.619983  \n",
       "1    6.779529   5.621259  \n",
       "2    0.097671   6.667539  \n",
       "3    8.010496   7.949886  \n",
       "4    6.551327   8.373261  \n",
       "5    8.246757   8.093122  "
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_frame = pd.DataFrame({'Model':models, \n",
    "                                          'Train_r2': r2_train,'Test_r2': r2_test,\n",
    "                                          'Train_RMSE':rmse_train,'Test_RMSE':rmse_test}) \n",
    "comparison_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZrq2E9VtOTD"
   },
   "source": [
    "#### **Observations: _____**\n",
    "The above comparison shows that the Decision Tree regression model has the best r2 (R-Squared) value of 0.999, and fits the best with both of training and test data with lowest Train_RMSE (0.097). The Next best is the Ridge-Lasso model with a R-squared value of 0.703 for test data, and relatively lower RMSE values (less than 7.0). The Tuned version of Random Forest regressor has the lowest R-Squared values, with high RMSE values for both of training and test data set, and hence this model is the worst fit. I feel that the estimators did not do a great job to choose the correct parameter values for tuning those models becasue tuned models had worse output than regular versions of those models (Decision Tree and Random Forest regressors).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acSjU_ZFvyVt"
   },
   "source": [
    "**Note:** You can also try some other algorithms such as kNN and compare the model performance with the existing ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58KMVhO_tOTD"
   },
   "source": [
    " **Insights**\n",
    "\n",
    "**Refined insights:**\n",
    "Data Insight is the deep understanding a person or an organization gains from analyzing data on a particular issue. This deep understanding helps the person or the organizations make better decisions rather than relying on personal instincts or guts. \n",
    "\n",
    "The issue here is improving used car pricing - how the best possible pricing could be made so that the company can make more and more profit selling used cars. The data analysis here using various regressors shows that the 5 Important features (Insights) are the Year, Power, Kilometers driven, Mileage, and Engine. One regression shows Manual Transmission type is an important factor as well. Knowing these factors, the company can better design a better pricing model. The decision tree regression seems the best in my experiments, and that shows the following Important Features with factors  -\n",
    "\n",
    "\n",
    "**Feature                    Factor**\n",
    "\n",
    "**---------------------------**\n",
    "\n",
    "**Power                      0.539645**\n",
    "\n",
    "**Year                       0.209081**\n",
    "\n",
    "**Kilometers_driven      0.074535**\n",
    "\n",
    "**Engine                     0.053221**\n",
    "\n",
    "**Mileage                    0.043895**\n",
    "\n",
    "**Transmission_Manual        0.008721**\n",
    "\n",
    "\n",
    "**Comparison of various techniques and their relative performance:**\n",
    "From my model design, fitting and scoring experiments, I find that the Decision Tree Regressor is the best in terms of R-squared value and values of Train_RMSE and Test_RMSE. This model has a R-squared value of 0.999 which is close to 1 and being a perfect score, meaning the best fit of data to the model. The Linear Regression model has the second best performance with a R-squared value of 0.7034, then the Ridge-Lasso model with similar metrics. The Random Forest models comes to the fourth position, and the tuned version of the Random Forest method has the worst performance, with a R-squared value of 0.352 for training data set and 0.385 for test data set. \n",
    "\n",
    "**Proposal for the final solution design:**\n",
    "I am going to propose using the Decision Tree Regression model for the solution of Prediction of Pricing for used cars. The R-squared value for this model impressed me. The Important Features indicated my this model is also very much practical; in other words, those matches with real life experience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "a1WHqIX9tOTC",
    "PBoHEXnjtOTC",
    "TZrq2E9VtOTD"
   ],
   "name": "Reference_Notebook_Milestone_2_Regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
